---
title: "Inleiding text mining in R"
author: "Longhow Lam"
subtitle: sessie 01
output:
  prettydoc::html_pretty:
    highlight: github
    theme: cayman
    toc: true
    toc_depth: 2
    number_sections: true
  html_notebook:
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 2
    toc_float: yes
---

---

```{r, eval=FALSE, include=FALSE}
library(stringr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(wordcloud)
library(wordcloud2)
library(text2vec)
library(pdftools)
library(reticulate)
library(cld2)
```

<br>

# Inleiding

---

Deze cursus geeft een inleiding in text mining in R. De volgende punten zullen worden behandeld: 

* String en text basics (stringr manipulaties, word clouds)
* Tokenization, lematization
* Het text2vec package
* Distance measures
* Latent Direchlet Allocation
* Latent semantic indexing
* GLMnet modellen
* Sentiment analyse
* word embeddings (glove)

We gebruiken in deze cursus een aantal data sets die ik eerder al eens gescraped heb. Het betreft: Iens, GTST, Jaap en Ajax, RTL en NOS nieuws.

Er wordt uitgegeaan van en basis kennis van R. Deze cursus bestaat uit twee sessies van ongeveer 3 uur. Er zijn wat begeleidende powerpoint slides.

<br>

# The basics

---

## Reguliere expressies en stringr

---

Character data in R kan je bewerken/manipuleren met het `stringr` package. Voordat we daar verder op in gaan is het handig om te weten wat reguliere expressies zijn.

### Reguliere expressies

Dit is een soort 'mini' taaltje om character patronen te specificeren om er vervolgens bijvoorbeeld op te zoeken. Eerst wat dummy character data.

```{r, eval=FALSE}
test = c("Mijn nummer is 06-12345678", "dit is .. een 1628EP postcode test", "foutje?:  0234XD", "dit is er nog een 1628 EP", "en nog een foute 126EP", "nog een 1234    XX", "1234eZ en nog 4567PK", "12345 Aabcde", "&yfy.")
test
```

Een paar voorbeelden van reguliere expressies.

```{r, eval=FALSE}
library(stringr)

## een digit, en precies 1 digit
patroon = "\\d"
str_extract(test, patroon)

## 1 of meerdere digits
patroon = "\\d+"
str_extract(test, patroon)

## precies twee letters
patroon ="[[:alpha:]]{2}"
str_extract(test, patroon)

## Een postcode
patroon = "\\d{4}\\s?[[:alpha:]]{2}"
str_extract(test, patroon)

## Maar een postcode begint niet met een 0
patroon = "[1-9]\\d{3}\\s?[[:alpha:]]{2}"
str_extract(test, patroon)

## special punctuation characters !"#$%&â€™()*+,-./:;<=>?@[]^_`{|}~
patroon = "[[:punct:]]"
str_extract(test, patroon)
```

Sommige mensen zijn de wildcard notatie gewend om te zoeken in strings, deze wildcard notatie kan je vertalen naar reguliere expressies met `glob2rx`. Een paar voorbeelden zullen hieronder geven. 

```{r, eval=FALSE}
patroon = glob2rx("*23*")
str_extract(test, patroon)
```

Voor een cheatsheet over reguliere expressies zie [regex cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf)

### functies uit het package stringr

In het package stringr zijn diverse functies om met characters te werken, een aantal zullen we laten zien

```{r, eval=FALSE}
testset = titanic::titanic_train %>% select(Name)
## maak extra kolom aan in titanic die namen zoekt
testset = testset %>%
  mutate(
    naamlengte = str_length(Name),
    Ownes = str_detect(Name, 'Owen'),
    Plek = str_locate(testset$Name, "Mr") %>% .[,1],
    Name2 = str_replace(Name, "Mr", "")
  )

str_locate(testset$Name, "Mr")
```

<br>


## wordclouds

Er zijn twee packages die je kan gebruiken voor wordclouds in R, `wordcloud` en `wordcloud2`

```{r, eval=FALSE}

ajax = readRDS("data/Ajax.Rds")
ajax = ajax %>% mutate_if(is.factor, as.character)

## pak 5 verslagen
verslagjes = paste(
  ajax$Verslag[1:3],
  collapse = " "
)

wordcounts = tibble(w = stringi::stri_extract_all_words(
  str_to_lower(verslagjes)
  ) %>%
  unlist
) %>%
group_by(w) %>%
summarise(n=n())

wordcloud(wordcounts$w, wordcounts$n)
wordcloud2(wordcounts)

```

Er zitten wat woorden in die we weg willen halen

```{r, eval=FALSE}

stopwoorden = c("ajax", "de", "een", "het","en","met","in", "van", "op", "er")

w = stringi::stri_extract_all_words(
      str_to_lower(verslagjes)
    ) %>%
    unlist

w =  w[!w %in% stopwoorden]


wordcounts2 = data.frame(w, stringsAsFactors = FALSE) %>%
group_by(w) %>%
summarise(n=n()) %>% filter (n>2)


wordcloud2(wordcounts2)

```

<br>


## Talen

```{r,eval=FALSE}
# installeer package als het nodig is
#devtools::install_github("ropensci/cld2")

cld2::detect_language("To be or not to be")
cld2::detect_language("ga toch weg, jij rotzak")
cld2::detect_language(url('https://www.rtlnieuws.nl/nederland/maastunnel-twee-jaar-dicht-van-noord-naar-zuid'), plain_text = FALSE)

```


## PDF files

```{r, eval=FALSE}
pdf_text("RestaurantsBlacklist/99.pdf")
```

# Tokenization , part-of-speech tagging en lemmatization

```{r, eval=FALSE}
library(udpipe)
dl <- udpipe_download_model(language = "dutch")
dl
```

```{r, eval=FALSE}
udmodel_dutch <- udpipe_load_model(file = "dutch-ud-2.0-170801.udpipe")
x <- udpipe_annotate(udmodel_dutch,  x = "Ik ging op reis en ik nam mee: mijn laptop, mijn zonnebril en goed humeur.")
x <- as.data.frame(x)
x
```

```{r, eval=FALSE}
udmodel_dutch <- udpipe_load_model(file = "dutch-ud-2.0-170801.udpipe")
x <- udpipe_annotate(udmodel_dutch, x = RTLN$bodie[1])
x <- as.data.frame(x)
x

```


# Het text2vec package

Het werkpaardje voor mij in R voor textmining
---


## iterators

prepare_fun

## vocabulairs

## term document matrices

## afstanden

## Collocations


## Termen prijzen in huis beschrijvingen

## Sentiment prediction

Ik maak onderscheid tussen voorgedefineerd sentiment met woorden lijsten, en voorspelling aan de hand van training data. 

### Nederlands voor gedefinieerd
Voor Nederlands kan je sentiment bepalen via het Python package pattern.nl (python 2.7 alleen), dit kan je in R aanroepen via reticulate

```{r}
library(reticulate)
pattern = import("pattern.nl") 
pattern$sentiment("Ik heb deze laptop gekocht wat een vreselijk slecht apparaat")

pattern$sentiment("Ik heb deze laptop gekocht. Vreselijk goed ding")
pattern$sentiment("Ik heb deze laptop gekocht, niet eens zo slecht")
```


### training sentiment

Voor het trainen van sentiment heb je gelabelde data nodig. per document heb je een score of POS/NEG label nodig. Ik gebruik hier als voorbeeld de IENS revies die ik een tijdje geleden heb gescrapet. Per review heb je ook een cijfer die de reviewer heeft gegeven. Deze gebruik ik als target.


```{r}
Iens = readRDS("data/IensReviews.rds")
```

Als eerste controleren op taal en voor deze analyse pakken we alleen de NL reviews.

```{r}
talen = detect_language(Iens$Review)
table(talen)
```


Doe glmnet direct op dtm

```{r}

```



De term document matrix is ontzetten sparse, om deze wat compacter te maken, zonder veel informatie verlies kunnen we een SVD trick toe passen.


Doe een svd en daarna een predictive model

![](SVD.png)

Als voorbeeldje kan je ook eens kijken naar mijn zzontje :-)   plaatje SVD..... uit mijn slide

```{r, eval=FALSE}
## enkele texten evatten wat rare characters
Sys.setlocale('LC_ALL','C')

it  = itoken(Iens$Review)
voc = create_vocabulary(it) %>% 
  prune_vocabulary(
    doc_proportion_max = 0.1,
    term_count_min = 5
  )

vectorizer = vocab_vectorizer(voc)
dtm = create_dtm(it, vectorizer)
dim(dtm)
```

De dtm is een vrij grote sparse matrix met veel nullen

```{r, eval = FALSE}
sum(dtm > 0) / (1.0*dim(dtm)[1]*dim(dtm)[2])
```

Pas de SVD toe en neem nu 50 dimensies

```{r, eval = FALSE}

tfidf = TfIdf$new()
lsa = LSA$new(n_topics = 50)

# pipe friendly transformation
doc_embeddings = dtm %>% 
  fit_transform(tfidf) %>% 
  fit_transform(lsa)

dim(doc_embeddings)
doc_embeddings[1:25,]
```



## glove word embeddings



Check mijn shiny app

http://145.131.21.163:3838/sample-apps/wordembeddings/

en code op mijn github


<br>

